---
title: 'Nrtsearch Tutorial - Indexing Web Content for Search'
date: '2023-04-12'
spoiler: Building a simple web crawler and website indexer. Using nrtsearch to support search with custom scoring and results highlighting for any website.
blogImages: []
---

Let's use [nrtsearch](https://github.com/Yelp/nrtsearch) - an open-source search engine built by Yelp - to support text search for any website. In this tutorial, I'm using my blog as an example dataset, but you can apply this approach to index any text content on the internet.

We will begin by building a simple web crawler to fetch your website's data. Then we will start a local nrtsearch cluster with a primary and replicas. We will design its index schema and set up an indexing pipeline to feed the data into the search engine. Finally, we will define nrtsearch queries to support custom scoring and highlighting. We will be able to interact with the system via a search UI. All of this will be done with minimal coding. I will use of open-source projects to keep things simple and extensible. You can clone [the tutorial repo](https://github.com/jedrazb/nrtsearch-tutorial-website-search) to run the code locally. All you need is `python3` and `docker` installed on your system.

If you want to learn more about nrtSearch you can read Yelp's Engineering blog post: [Nrtsearch: Yelpâ€™s Fast, Scalable and Cost Effective Search Engine](https://engineeringblog.yelp.com/2021/09/nrtsearch-yelps-fast-scalable-and-cost-effective-search-engine.html). Let's start!

## Web crawler

The crawler fetches data from websites. In this tutorial I'm using my blog as an example dataset. For simplicity, I supplied a sitemap - a list of urls - to the crawler so that it can directly fetch the data.

Let's use [beautifulsoup](https://beautiful-soup-4.readthedocs.io/en/latest/) library for extracting website content. The website data can be represented in a following way:

- `title` - website title tag
- `description` - meta description tag, a short summary of the website, provided by the author
- `url`
- `content`- extracted text content, e.g. paragraphs
- `headings` - extracted website's headings. For simplicity we group `h1`, `h2`, `h3`, ... tags together.

Our code uses `get_source_urls()` function to get the list of urls. The function will process the HTML pages to extract: title, description, content and headings. After it's done, it will save the output to a JSON file.

```python
import json
import requests
from bs4 import BeautifulSoup

urls = get_source_urls() # urls from website sitemap

website_data = []

for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    title = soup.title.text.strip() if soup.title else None
    description = (
        soup.find("meta", attrs={"name": "description"})["content"].strip()
        if soup.find("meta", attrs={"name": "description"})
        else None
    )
    headings = [
        h.text.strip() for h in soup.find_all(["h1", "h2", "h3", "h4", "h5"])
    ]
    content = soup.get_text(" ", strip=True)

    website_data.append(
        {
            "url": url,
            "title": title,
            "description": description,
            "headings": headings,
            "content": content,
        }
    )

with open('index_resources/website_data.json', 'w') as outfile:
    json.dump(website_data, outfile)

```

The full crawler code is located in [crawler/crawler.py](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/crawler/crawler.py), you can run the crawler by executing:

```bash
make run_crawler
```

## Starting the nrtsearch cluster

The nrtsearch cluster consists of 2 types of nodes:

- `primary` - a single node, responsible for data indexing. It periodically publishes Lucene segment updates to replicas. Hence the name: _nrtsearch - near-real time search_.
- `replica` - one or more nodes, responsible for serving the search traffic. It receives periodic segment updates from the primary. The number of running replicas can be controlled by an auto-scaler (like [HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)).

It's also possible to run the cluster with a single, standalone node, used for both data ingestion and search. It's ok for experimentation but for production environments it's recommended to run separate primary and replica nodes.

The source code for nrtsearch setup is available in [/nrtsearch](https://github.com/jedrazb/nrtsearch-tutorial-website-search/tree/master/nrtsearch) folder. There we have:

- `Dockerfile` to setup an nrtsearch node
- configs for primary and replicas
- `docker-compose.yaml` to setup a local cluster with a single primary and two replicas

Note: in this tutorial we don't care about persisting the index state. If you recreate the cluster, with docker compose, all state will be lost. In order to persist your index, you can attach a permanent volume. For production, it's advisable to use a dedicated s3 bucket where nrtsearch will store the cluster state. This can be easily setup in the configuration file. More about it in the [docs](https://nrtsearch.readthedocs.io/en/latest/server_configuration.html).

In order to start the local cluster with a primary and two replicas, run a following command in a separate terminal window:

```bash
make start_nrtsearch_cluster
```

Above command will run `docker compose up` that takes the primary/replica configuration from [docker-compose.yaml](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/nrtsearch/docker-compose.yaml) file. In your terminal window you should see logs indicating that the primary and replicas are ready, and the server has started:

```bash
primary-node              | [INFO ] 2023-04-11 08:32:16.712 [main] LuceneServer - Server started, listening on 8000 for messages
```

When running `docker compose ps`, you should see the following output. Note that the gRPC server of each container (`8000` container port) is accessible on a different port on the host machine. We use port `8000` for the primary node and ports `8001` and `8002` for the two replicas.

```bash
NAME                       IMAGE                    COMMAND                  SERVICE             CREATED             STATUS              PORTS
nrtsearch-replica-node-1   nrtsearch-replica-node   "./entrypoint_replicâ€¦"   replica-node        12 minutes ago      Up 12 minutes       0.0.0.0:8002->8000/tcp
nrtsearch-replica-node-2   nrtsearch-replica-node   "./entrypoint_replicâ€¦"   replica-node        12 minutes ago      Up 12 minutes       0.0.0.0:8001->8000/tcp
primary-node               nrtsearch-primary-node   "bash -c '/user/nrtsâ€¦"   primary-node        12 minutes ago      Up 12 minutes       0.0.0.0:8000->8000/tcp
```

Great! Now we have a local nrtsearch cluster with nodes dedicated to data ingestion and search. Even though the containers are now running, no index was created. A new index must be created explicitly via a command. Once the index started is started in replicas, they register with the primary. Replication happens only after documents are ingested into the primary.

Now, letâ€™s prepare the client code to interact with our cluster.

## Generating gRPC client code using protoc

Clients can communicate with nrtsearch via gRPC or REST API. [gRPC](https://grpc.io/) is a high-performance remote procedure call framework that enables communication by using protocol buffers for serialization and deserialization of messages. This method yields great performance for production applications. The REST API is an optional way to communicate with nrtsearch, good for quick development and experimentation. The REST server ([grpc-gateway](https://github.com/grpc-ecosystem/grpc-gateway)) is autogenerated from `proto` definitions.

Let's use gRPC to communicate with nrtsearch. Since it's not a REST API we can't just `curl` the endpoints. We need to generate the native client code, `python` for this tutorial, to interact with nrtsearch.

To generate Python client code from proto definitions, you can use the gRPC toolchain's `protoc` compiler along with the `grpc_tools` package to compile the `.proto` files into Python code, and then use the generated client code to communicate with the gRPC server. The steps are:

- get all nrtsearch `.proto` files along with their dependencies to a folder, let's call it `/protos`
- point `protoc` compiler to `/protos` folder and generate the python code

In the [Makefile](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/Makefile) there are commands `nrtsearch_protos` and `nrtsearch_protos` which will execute the steps above and put the generated code into [/nrtsearch_client/nrtsearch_py_grpc](https://github.com/jedrazb/nrtsearch-tutorial-website-search/tree/master/nrtsearch_client/nrtsearch_py_grpc).

```makefile
# Generate nrtsearch .proto files and their dependencies
nrtsearch_protos:
	mkdir -p protos
	docker build -t nrtsearch-protos-builder:latest ./utils/nrtsearch_protos_builder/
	docker run -v $(shell pwd)/protos:/user/protos  nrtsearch-protos-builder:latest

# Compile client .proto files to python code
nrtsearch_protoc: nrtsearch_protos
	$(PYTHON) -m grpc_tools.protoc \
		--proto_path protos \
		--grpc_python_out nrtsearch_client \
		--python_out nrtsearch_client \
		protos/yelp/nrtsearch/luceneserver.proto \
		protos/yelp/nrtsearch/search.proto \
		protos/yelp/nrtsearch/analysis.proto \
		protos/yelp/nrtsearch/suggest.proto
	rm -rf protos
```

In order to nrtsearch `.proto` files and their dependencies we use a simple [Dockerfile](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/utils/nrtsearch_protos_builder/Dockerfile) that compiles the project and moves all proto files into the persistent volume `/protos`. Then protoc compiler compiles them to python code. After running `make nrtsearch_protoc` you can verify that the client code was generated successfully:

```bash
ls nrtsearch_client/yelp/nrtsearch/
analysis_pb2.py          luceneserver_pb2.py      search_pb2.py            suggest_pb2.py
analysis_pb2_grpc.py     luceneserver_pb2_grpc.py search_pb2_grpc.py       suggest_pb2_grpc.py
```

## Index schema

In order to ingest the data into nrtsearch, we must first create an index and register the fields. It is important to define appropriate analyzers where necessary in order to ensure that Lucene's search performance is optimal.

For our tutorial index `blog_search` we need to index the data returned from [the crawler](/tech-blog/nrtsearch-tutorial-website-search/#web-crawler): url, title, description, headings and text content.

- We use `TEXT` type for each field, as this allows to the data to be tokenized and index to Lucene with index-time analyzer.
- The nested field `url.id` has `_ID` type - it defines the unique Lucene document id. Note, the `url` field has `TEXT` type as we still want to keyword search within the url
- Each field is searchable (`search: true`), tokenized (`tokenize: true`) and is stored in column-oriented way (`storeDocValues: true`).
- The `content.analyzed` field contains the index-time analyzed webpage content. We define a custom analyzer which removes any html characters (`htmlstrip` charFilter), uses `standard` tokenizer and `kStem`, `stop` and `lowercase` Lucene token filters. In order to support the [Fast Vector Highlighter](https://nrtsearch.readthedocs.io/en/latest/highlighting.html) for results highlighting, we set `termVectors`, `store` and `tokenize` properties.

The `blog_search` index schema is defined in [/index_resources/index_schema.yaml](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/index_resources/index_schema.yaml) and looks like this:

```yaml
indexName: blog_search
field:
  - name: url
    type: TEXT
    search: true
    storeDocValues: true
    tokenize: true
    childFields:
      - name: id
        type: _ID
        search: true
        storeDocValues: true
  - name: title
    type: TEXT
    search: true
    storeDocValues: true
    tokenize: true
  - name: description
    type: TEXT
    search: true
    storeDocValues: true
    tokenize: true
  - name: headings
    type: TEXT
    search: true
    storeDocValues: true
    tokenize: true
    multiValued: true
  - name: content
    type: TEXT
    search: true
    storeDocValues: true
    tokenize: true
    store: true
    termVectors: TERMS_POSITIONS_OFFSETS
    childFields:
      - name: analyzed
        type: TEXT
        search: true
        store: true
        termVectors: TERMS_POSITIONS_OFFSETS
        tokenize: true
        analyzer:
          custom:
            charFilters:
              - name: htmlstrip
            tokenizer:
              name: standard
            tokenFilters:
              - name: lowercase
              - name: kStem
              - name: stop
```

More information about the nrtsearch field types, schema design and supported analyzers is available in the [nrtsearch docs](https://nrtsearch.readthedocs.io/en/latest/analysis.html).

## Creating the nrtsearch index

Now it's time to create and start an index for blog data. We have a running nrtsearch cluster, generated client code and the index schema.

In order to configure the index we need to run several requests in order. We need to configure primary and replicas, to ensure that replicas register themselves with the primary for index replication. We need to run commands in order, for the primary and each of the replicas:

1. [CreateIndexRequest](https://github.com/Yelp/nrtsearch/blob/master/clientlib/src/main/proto/yelp/nrtsearch/luceneserver.proto#L428) - creates the nrtsearch index with a given name
2. [SettingsRequest](https://github.com/Yelp/nrtsearch/blob/master/clientlib/src/main/proto/yelp/nrtsearch/luceneserver.proto#L594) - sets Lucene-related index settings
3. [StartIndexRequest](https://github.com/Yelp/nrtsearch/blob/master/clientlib/src/main/proto/yelp/nrtsearch/luceneserver.proto#L628) - starts the index in a given mode (`STANDALONE`, `PRIMARY` or `REPLICA`), additionally for replicas we need to specify primary `host:port` for index replication
4. [FieldDefRequest](https://github.com/Yelp/nrtsearch/blob/master/clientlib/src/main/proto/yelp/nrtsearch/luceneserver.proto#L583) - registers index schema
5. [CommitRequest](https://github.com/Yelp/nrtsearch/blob/master/clientlib/src/main/proto/yelp/nrtsearch/luceneserver.proto#L687) - in production it's recommended to commit the newly created index. It ensures its state is preserved in s3 or other persistent storage

Each docker compose container has assigned a fixed `host:port` so I'm using dummy service discovery to discover and communicate with nrtsearch nodes:

```python
# Simple service discovery
SERVICE_DISCOVERY = {
    "primary-node": ("0.0.0.0", 8000),
    "replica-node-0": ("0.0.0.0", 8001),
    "replica-node-1": ("0.0.0.0", 8002),
}
```

In [/nrtsearch_client/client.py](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/nrtsearch_client/client.py) we have the gRPC client which will communicate with our cluster nodes:

```python
@lru_cache(3)
def get_nrtsearch_client(host, port):
    channel = grpc.insecure_channel(f"{host}:{port}")
    return LuceneServerStub(channel)
```

The code in [/nrtsearch_client/setup_index.py](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/nrtsearch_client/setup_index.py) will use the service discovery to create clients for each node and execute above commands in order. Generated client code allows for sending gRPC messages to nrtsearch. You can run the code for index setup locally via `make setup_index`.

```python
INDEX_NAME = "blog_search"
INDEX_SCHEMA_PATH = "index_resources/index_schema.yaml"
INDEX_SETTINGS_PATH = "index_resources/index_settings.yaml"


def create_index(nrtsearch_client):
    response = nrtsearch_client.createIndex(CreateIndexRequest(indexName=INDEX_NAME))
    return response


def apply_index_settings(nrtsearch_client, settings_dict):
    settings_request = Parse(json.dumps(settings_dict), SettingsRequest())
    response = nrtsearch_client.settings(settings_request)
    return response


def start_index_primary(nrtsearch_primary_client):
    response = nrtsearch_primary_client.startIndex(
        StartIndexRequest(indexName=INDEX_NAME, mode=1)  # PRIMARY
    )
    return response


def start_index_replica(
    nrtsearch_replica_client,
):
    response = nrtsearch_replica_client.startIndex(
        StartIndexRequest(
            indexName=INDEX_NAME,
            mode=2,  # REPLICA
            primaryAddress="primary-node",  # docker address, can be resolved within the container
            port=8001,  # replication port
        )
    )
    return response


def register_fields(nrtsearch_client, index_schema_dict):
    register_fields_req = Parse(json.dumps(index_schema_dict), FieldDefRequest())
    response = nrtsearch_client.registerFields(register_fields_req)
    return response


def commit_index(nrtsearch_client):
    response = nrtsearch_client.commit(CommitRequest(indexName=INDEX_NAME))
    return response
```

The `start_index_replica` starts the index in `REPLICA` mode and registers to primary to listen for index updates. The `primaryAddress="primary-node"` is defined in [docker-compose.yaml](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/nrtsearch/docker-compose.yaml#L9) and the `8001` replication port comes from the primary [server configuration](https://github.com/jedrazb/nrtsearch-tutorial-website-search/blob/master/nrtsearch/nrtsearch-primary-config.yaml#L4).

You can verify the new index is created on each node by running `venv/bin/python nrtsearch_client/get_indices.py`, the output should look like:

```bash
IndicesRequest for: primary-node
indicesResponse {
  indexName: "blog_search"
  statsResponse {
    dirSize: 96
    state: "started"
    taxonomy {
    }
    currentSearcher {
      segments: "IndexSearcher(StandardDirectoryReader(segments_1:2:nrt); executor=ExecutorWithParams(sli
ceMaxDocs=250000, sliceMaxSegments=5, virtualShards=1, wrapped=java.util.concurrent.ThreadPoolExecutor@4a
bd0a9d[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]))"
    }
  }
}

IndicesRequest for: replica-node-0
indicesResponse {
    #...
}
```

Great! Now we have a running nrtsearch cluster and our index is started and replicated between primary and replicas. Let's move on to data ingestion.

## nrtSearch indexer

In order to ingest data into nrtsearch we can use the [addDocuments](https://github.com/Yelp/nrtsearch/blob/master/clientlib/src/main/proto/yelp/nrtsearch/luceneserver.proto#L135) client method. It allows streaming multiple `AddDocumentRequest` to the primary for indexing.

A single call to `addDocuments` can contain multiple documents in the payload. You can call it a bulk update. Itâ€™s more efficient when indexing a large amounts of documents. Therefore, it's advisable to batch your documents and ingest them in bulk for production use-cases.

> There is no transaction log, so you must call commit periodically to make recent changes durable on disk. This means that if a node crashes, all indexed documents since the last commit are lost. ([docs](https://nrtsearch.readthedocs.io/en/latest/introduction.html#design))

When ingesting our data to nrtsearch we need to call `commit` periodically to ensure our data is persisted. It works very well with streaming frameworks, like Flink, where you can call `commit` every time your Flink application checkpoints - and the checkpoint is successful if and only if the commit call succeeds.

We will use a python script, which will batch docs and ingest them into the primary and commit the index. We will be able verify the state of replicas to check if index segments were replicated.

```python
# For prod cases we can even make it 1000, taking small value for this tutorial
BATCH_SIZE = 10


# Load the data processed and saved by the crawler
def _load_website_data():
    with open("index_resources/website_data.json") as f:
        data = json.load(f)

    return data


# Prepare the request payload
def _prepare_document_stream(docs):
    for doc in docs:
        fields = {
            "url": AddDocumentRequest.MultiValuedField(value=[doc["url"]]),
            "title": AddDocumentRequest.MultiValuedField(value=[doc["title"]]),
            "description": AddDocumentRequest.MultiValuedField(
                value=[doc["description"]]
            ),
            "headings": AddDocumentRequest.MultiValuedField(value=doc["headings"]),
            "content": AddDocumentRequest.MultiValuedField(value=[doc["content"]]),
        }

        yield AddDocumentRequest(indexName=INDEX_NAME, fields=fields)


# Use gRPC streaming to send documents for ingestion
def index_document_stream(primary_client, doc_stream):
    response = primary_client.addDocuments(doc_stream)
    return response


def run_indexer():
    # Just index data to primary - data will be replicated to other nodes in the cluster
    host, port = SERVICE_DISCOVERY.get("primary-node")
    primary_client = get_nrtsearch_client(host, port)

    website_docs = _load_website_data()

    # Split data into chunks for bulk ingestion
    chunked_website_docs = chunked(website_docs, BATCH_SIZE)

    # Process each chunk of docs in bulk
    for idx, docs_chunk in enumerate(chunked_website_docs):
        doc_stream = _prepare_document_stream(docs=docs_chunk)
        index_response = index_document_stream(primary_client, doc_stream)

    # Call commit after docs are ingested
    commit_response = commit_index(primary_client)
```

You can ingest the data by running `make run_indexer`. After the script is done, let's check the number of docs in each nrtsearch node - to verify that updates were propagated to replicas.

```bash
> venv/bin/python nrtsearch_client/get_indices.py
IndicesRequest for: primary-node
indicesResponse {
  indexName: "blog_search"
  statsResponse {
    maxDoc: 21
    numDocs: 21
    ...
    currentSearcher {
      numDocs: 21
      ...
    }
  }
}
...
IndicesRequest for: replica-node-1
indicesResponse {
  indexName: "blog_search"
  statsResponse {
    ...
    }
    searchers {
      numDocs: 21
      ...
    }
    ...
  }
}
```

We can see that data was successfully replicated between primary and replica nodes - both report 21 docs in their index. Great! ðŸŽ‰

## Custom search scoring

## Highlighting support

## Simple search UI

## Putting it all together

## Index your data
