---
title: 'Running Apache Lucene at Scale'
date: '2023-03-18'
spoiler: 'Running Apache Lucene at scale - overview of Elasticsearch and NrtSearch.'
blogImages: []
---

In 2020, I joined Yelp as a software engineer on the Ranking Platform team. The team is responsible for Yelp's search and ranking infrastructure, which powers the business and reviews search and internal real-time ad bidding system.

At that time, I participated in an initiative to overhaul our core search and ranking infrastructure. Our goal was to improve performance and cost efficiency, and the work resulted in the creation of an open-source search engine - [nrtSearch](https://github.com/Yelp/nrtsearch). As of 2023, nrtSearch is now the primary search and ranking engine used for the majority of Yelp's search use cases.

Having the benefit of being a heavy nrtSearch user for the past 2 years I want to give a brief overview from the

This post concludes

- [Exploring Apache Lucene - Part 1: The Index](/tech-blog/exploring-apache-lucene-index/)
- [Exploring Apache Lucene - Part 2: Search and Ranking](/tech-blog/exploring-apache-lucene-search-and-ranking/)

# Lucene at scale

As search volumes and datasets grow, maintaining high performance with Apache Lucene can become increasingly challenging. As a result, scaling Lucene-based search engines becomes a crucial consideration for data-driven businesses. There are several popular implementations that can help to achieve this goal, such as Elasticsearch, Solr, and nrtSearch.

They all allow to scale out Lucene across multiple nodes, with an added API wrapper for data ingestion and search. But, their different designs and architectures make the problem of running Lucene at scale quite interesting. While they all aim to solve the same problem, their specific implementation may have different tradeoffs in terms of performance, scalability, fault tolerance, and ease of use.

Distributing and replicating an index across multiple nodes are common approaches for achieving high scalability and performance in search applications.

## Data distribution

If your searches are starting to take longer or your index is getting too large for a node to handle, it might be time to consider distributing your data across multiple nodes.

To do this, you can divide your index into smaller partitions, called shards, which can then be distributed across separate nodes. A search engine can partition a search query into sub-searches that are run on each shard, and then the results are combined - this is known as _Scatter-gather_.

The underlying technical details of index sharding are typically invisible to end-users. They'll simply experience faster search performance, especially when working with very large indexes. By distributing your data, you can overcome physical limitations and improve search efficiency.

## Data replication

When you have a large search volume that can't be handled by a single node, you can distribute searches across multiple read-only copies of the index to improve search performance. By replicating the index, search queries can be processed concurrently across multiple nodes, resulting in faster response times for end-users. This makes it possible to handle large volumes of data and queries, which is critical for building a high-performing, scalable search engine.

In scenario of a high volume of indexing that consumes resources and reduces search performance on the indexing node, you can separate indexing and searching by replicating the index. In this context _"replicating"_ can mean e.g. transferring index segments to replica nodes responsible for search. This allows you to distribute indexing and searching across different nodes, which can help to improve search performance and reduce the load on any one node. By separating indexing and searching, you can ensure that your search engine remains performant and responsive, even as your indexing needs grow.

# Elasticsearch

> [Elasticsearch from the Top Down](https://www.elastic.co/blog/found-elasticsearch-top-down)

### Cluster

A cluster in Elasticsearch is a collection of nodes that work together to serve search requests and maintain the cluster state. Each node in the cluster is identified by a unique name and has a specific role, such as data node, master-eligible node, or coordinating node.

### Shards

Elasticsearch uses the concept of shards to distribute the data across the nodes in the cluster. A shard is a subset of the index data, and each shard can be hosted on a different node in the cluster. This allows Elasticsearch to scale horizontally by adding more nodes to the cluster and distributing the data across them.

### Primaries and Replicas

Each shard has one primary shard, which is responsible for all write operations and indexing. The primary shard is also responsible for distributing the data to its replica shards.

To ensure high availability and fault tolerance, Elasticsearch also allows for the creation of replica shards. A replica is a copy of a shard that is hosted on a different node than the primary shard. This provides redundancy in case a node or shard fails, and allows Elasticsearch to continue serving search requests even if some nodes are offline.

### Request Coordinators

To handle search requests, Elasticsearch uses coordinating nodes, which act as a gateway to the rest of the cluster. When a search request is received, the coordinating node forwards the request to the appropriate shards and aggregates the results. Coordinating nodes do not hold any data or participate in indexing, but they play a critical role in scaling Elasticsearch's search capabilities.

# Nrtsearch

You can read more about the nrtSearch project results in the blog post from Yelp’s Engineering Blog:

> [Nrtsearch: Yelp’s Fast, Scalable and Cost Effective Search Engine](https://engineeringblog.yelp.com/2021/09/nrtsearch-yelps-fast-scalable-and-cost-effective-search-engine.html)
