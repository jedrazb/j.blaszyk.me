"use strict";(self.webpackChunkj_blaszyk_me=self.webpackChunkj_blaszyk_me||[]).push([[186],{3409:function(e,n,a){a.r(n),a.d(n,{default:function(){return H}});var t=a(7387),s=a(8453),o=a(6540);function r(e){const n=Object.assign({p:"p",span:"span",h2:"h2",a:"a",ul:"ul",li:"li",strong:"strong",h3:"h3",blockquote:"blockquote"},(0,s.R)(),e.components),{ImageComponent:a}=n;return a||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("ImageComponent",!0),o.createElement(o.Fragment,null,o.createElement(n.p,null,"Let’s build a retrieval-augmented generation (RAG) pipeline powered by a LLM that can run locally in a notebook. We’ll use the content of this blog as our sample knowledge source. By leveraging ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">langchain</code>'}})," and HuggingFace’s ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">transformers</code>'}})," packages, I aim to keep the code minimal and utilize their elegant abstractions."),"\n",o.createElement(n.p,null,"For our vector store, I’m using Elasticsearch Serverless (currently in open preview), which will enable semantic search on embedded textual data. You can run the RAG pipeline locally in your notebook, linked at the end of this post."),"\n",o.createElement(n.h2,{id:"high-level-architecture",style:{position:"relative"}},o.createElement(n.a,{href:"#high-level-architecture","aria-label":"high level architecture permalink",className:"anchor before"},o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"High-level architecture"),"\n",o.createElement(a,{image:e.data.mdx.frontmatter.images[0],description:"A high-level overview of the question-answering system involves informational retrieval using a vector store and a language model to generate answers based on the retrieved context."}),"\n",o.createElement(n.ul,null,"\n",o.createElement(n.li,null,o.createElement(n.strong,null,"Question"),": The user input question in natural language."),"\n",o.createElement(n.li,null,o.createElement(n.strong,null,"Retriever"),": A search engine that retrieves relevant documents from the knowledge source and passes them as context to the generator."),"\n",o.createElement(n.li,null,o.createElement(n.strong,null,"Knowledge Source"),": A repository of documents where their vector representations are stored, serving as the database for the retriever to search for answers."),"\n",o.createElement(n.li,null,o.createElement(n.strong,null,"Generator"),": A language model that generates answers based on the question and the retrieved context."),"\n",o.createElement(n.li,null,o.createElement(n.strong,null,"Answer"),": The generated answer to the user’s question in natural language."),"\n"),"\n",o.createElement(n.h2,{id:"embeddings",style:{position:"relative"}},o.createElement(n.a,{href:"#embeddings","aria-label":"embeddings permalink",className:"anchor before"},o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Embeddings"),"\n",o.createElement(n.p,null,"We are using default ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">HuggingFaceEmbeddings</code>'}})," to encode documents into 768-dimensional dense vectors. These embeddings are generated using a pre-trained ",o.createElement(n.a,{href:"https://huggingface.co/sentence-transformers/all-mpnet-base-v2",target:"_blank",rel:"nofollow noopener noreferrer"},"sentence-transformers/all-mpnet-base-v2")," model."),"\n",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>embeddings <span class="token keyword">import</span> HuggingFaceEmbeddings\n\nembeddings <span class="token operator">=</span> HuggingFaceEmbeddings<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre></div>'}}),"\n",o.createElement(n.h2,{id:"knowledge-source",style:{position:"relative"}},o.createElement(n.a,{href:"#knowledge-source","aria-label":"knowledge source permalink",className:"anchor before"},o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Knowledge source"),"\n",o.createElement(n.p,null,"We are using the content of this blog as a knowledge source for our toy example."),"\n",o.createElement(n.h3,{id:"crawling-blog-content",style:{position:"relative"}},o.createElement(n.a,{href:"#crawling-blog-content","aria-label":"crawling blog content permalink",className:"anchor before"},o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Crawling blog content"),"\n",o.createElement(n.p,null,"I used the Elastic web crawler to crawl the blog content. Then, I scanned the Elasticsearch index and saved the ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">_source</code>'}})," of docs in a JSONL file: ",o.createElement(n.a,{href:"https://gist.github.com/jedrazb/0c9df82143b694147e7b018370508535",target:"_blank",rel:"nofollow noopener noreferrer"},"blog-pages.jsonl"),"."),"\n",o.createElement(n.p,null,"The ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">JSONLoader</code>'}})," is used to load documents from the file and extract metadata. The ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">content_key</code>'}})," specifies the key in the JSON record that contains the textual content of the document, which will be embedded as a vector for semantic search."),"\n",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>document_loaders <span class="token keyword">import</span> JSONLoader\n\n<span class="token keyword">def</span> <span class="token function">metadata_func</span><span class="token punctuation">(</span>record<span class="token punctuation">,</span> metadata<span class="token punctuation">)</span><span class="token punctuation">:</span>\n    metadata<span class="token punctuation">[</span><span class="token string">"url"</span><span class="token punctuation">]</span> <span class="token operator">=</span> record<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"url"</span><span class="token punctuation">)</span>\n    metadata<span class="token punctuation">[</span><span class="token string">"title"</span><span class="token punctuation">]</span> <span class="token operator">=</span> record<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"title"</span><span class="token punctuation">)</span>\n    metadata<span class="token punctuation">[</span><span class="token string">"links"</span><span class="token punctuation">]</span> <span class="token operator">=</span> record<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"links"</span><span class="token punctuation">)</span>\n    metadata<span class="token punctuation">[</span><span class="token string">"meta_description"</span><span class="token punctuation">]</span> <span class="token operator">=</span> record<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"meta_description"</span><span class="token punctuation">)</span>\n\n    <span class="token keyword">return</span> metadata\n\n\nloader <span class="token operator">=</span> JSONLoader<span class="token punctuation">(</span>\n    file_path<span class="token operator">=</span><span class="token string">\'./blog-pages.jsonl\'</span><span class="token punctuation">,</span>\n    jq_schema<span class="token operator">=</span><span class="token string">\'.\'</span><span class="token punctuation">,</span>\n    content_key<span class="token operator">=</span><span class="token string">"body_content"</span><span class="token punctuation">,</span>\n    metadata_func<span class="token operator">=</span>metadata_func<span class="token punctuation">,</span>\n    json_lines<span class="token operator">=</span><span class="token boolean">True</span>\n<span class="token punctuation">)</span></code></pre></div>'}}),"\n",o.createElement(n.h3,{id:"document-chunking",style:{position:"relative"}},o.createElement(n.a,{href:"#document-chunking","aria-label":"document chunking permalink",className:"anchor before"},o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Document chunking"),"\n",o.createElement(n.p,null,"Language models have token limits, so chunking breaks down large documents into smaller, manageable pieces that fit within these limits. Larger chunks capture more context but may lead to inefficiency or truncation, while smaller chunks are better for technical or dense content."),"\n",o.createElement(n.p,null,"When selecting ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">chunk_size</code>'}}),", choose a size within the model’s token limit. For ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">chunk_overlap</code>'}}),", select a value that preserves context across chunk boundaries."),"\n",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>document_loaders <span class="token keyword">import</span> TextLoader\n<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>text_splitter <span class="token keyword">import</span> RecursiveCharacterTextSplitter\n\ntext_splitter <span class="token operator">=</span> RecursiveCharacterTextSplitter<span class="token punctuation">.</span>from_tiktoken_encoder<span class="token punctuation">(</span>\n    chunk_size<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> chunk_overlap<span class="token operator">=</span><span class="token number">64</span>\n<span class="token punctuation">)</span>\ndocs <span class="token operator">=</span> loader<span class="token punctuation">.</span>load_and_split<span class="token punctuation">(</span>text_splitter<span class="token operator">=</span>text_splitter<span class="token punctuation">)</span></code></pre></div>'}}),"\n",o.createElement(n.h2,{id:"vector-store---elasticsearch-serverless",style:{position:"relative"}},o.createElement(n.a,{href:"#vector-store---elasticsearch-serverless","aria-label":"vector store   elasticsearch serverless permalink",className:"anchor before"},o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Vector store - Elasticsearch Serverless"),"\n",o.createElement(n.p,null,"I’m ingesting embedded data into ",o.createElement(n.a,{href:"https://docs.elastic.co/serverless",target:"_blank",rel:"nofollow noopener noreferrer"},"Elasticsearch Serverless"),", which will serve as our vector store. This service allows you to deploy and use Elastic without managing the underlying cluster, including nodes, data tiers, and scaling. Serverless instances are fully managed, autoscaled, and automatically upgraded by Elastic. It’s currently in technical preview and free to use."),"\n",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">ES_VECTOR_FIELD <span class="token operator">=</span> <span class="token string">\'content_embedding\'</span>\nES_INDEX_NAME <span class="token operator">=</span> <span class="token string">\'blog-search-qa\'</span></code></pre></div>'}}),"\n",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> langchain_elasticsearch <span class="token keyword">import</span> ElasticsearchStore\n\nElasticsearchStore<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>\n    docs<span class="token punctuation">,</span>\n    embeddings<span class="token punctuation">,</span>\n    vector_query_field<span class="token operator">=</span>ES_VECTOR_FIELD<span class="token punctuation">,</span>\n    es_url<span class="token operator">=</span>es_url<span class="token punctuation">,</span>\n    index_name<span class="token operator">=</span>ES_INDEX_NAME<span class="token punctuation">,</span>\n    es_api_key<span class="token operator">=</span>es_api_key\n<span class="token punctuation">)</span></code></pre></div>'}}),"\n",o.createElement(n.h2,{id:"retrieval-with-vector-search",style:{position:"relative"}},o.createElement(n.a,{href:"#retrieval-with-vector-search","aria-label":"retrieval with vector search permalink",className:"anchor before"},o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Retrieval with vector search"),"\n",o.createElement(n.p,null,"A ",o.createElement(n.a,{href:"https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/",target:"_blank",rel:"nofollow noopener noreferrer"},"retriever")," is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them."),"\n",o.createElement(n.p,null,"In our case, we are using pure vector search for retrieval. However, refer to the ",o.createElement(n.a,{href:"https://python.langchain.com/v0.1/docs/integrations/retrievers/elasticsearch_retriever/",target:"_blank",rel:"nofollow noopener noreferrer"},"ElasticsearchRetriever documentation")," for examples on how to configure a retriever with BM25 or hybrid search."),"\n",o.createElement(n.p,null,"In our kNN query, we use the ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">k</code>'}})," parameter to specify the number of nearest neighbors to retrieve. The ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">num_candidates</code>'}})," parameter determines the number of candidates considered during the nearest neighbor search."),"\n",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> langchain_elasticsearch <span class="token keyword">import</span> ElasticsearchRetriever\n\n<span class="token keyword">def</span> <span class="token function">vector_query</span><span class="token punctuation">(</span>search_query<span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>\n    <span class="token keyword">return</span> <span class="token punctuation">{</span>\n        <span class="token string">"knn"</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>\n            <span class="token string">"field"</span><span class="token punctuation">:</span> ES_VECTOR_FIELD<span class="token punctuation">,</span>\n            <span class="token string">"query_vector"</span><span class="token punctuation">:</span> embeddings<span class="token punctuation">.</span>embed_query<span class="token punctuation">(</span>search_query<span class="token punctuation">)</span><span class="token punctuation">,</span>\n            <span class="token string">"k"</span><span class="token punctuation">:</span> k<span class="token punctuation">,</span>\n            <span class="token string">"num_candidates"</span><span class="token punctuation">:</span> <span class="token number">20</span><span class="token punctuation">,</span>\n        <span class="token punctuation">}</span>\n    <span class="token punctuation">}</span>\n\n\nretriever <span class="token operator">=</span> ElasticsearchRetriever<span class="token punctuation">.</span>from_es_params<span class="token punctuation">(</span>\n    index_name<span class="token operator">=</span>ES_INDEX_NAME<span class="token punctuation">,</span>\n    body_func<span class="token operator">=</span>vector_query<span class="token punctuation">,</span>\n    content_field<span class="token operator">=</span><span class="token string">"text"</span><span class="token punctuation">,</span>\n    url<span class="token operator">=</span>es_url<span class="token punctuation">,</span>\n    api_key<span class="token operator">=</span>es_api_key\n<span class="token punctuation">)</span></code></pre></div>'}}),"\n",o.createElement(n.h2,{id:"question-answering",style:{position:"relative"}},o.createElement(n.a,{href:"#question-answering","aria-label":"question answering permalink",className:"anchor before"},o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Question answering"),"\n",o.createElement(n.p,null,"Let’s use a pre-trained question-answering model to extract answers from a set of context documents. Using the ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">transformers</code>'}})," library, we initialize a question-answering pipeline with the ",o.createElement(n.a,{href:"https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad",target:"_blank",rel:"nofollow noopener noreferrer"},"distilbert/distilbert-base-cased-distilled-squad")," model. This model is an efficient and lightweight Transformer model trained by distilling BERT base, and it is fine-tuned on ",o.createElement(n.a,{href:"https://huggingface.co/datasets/rajpurkar/squad",target:"_blank",rel:"nofollow noopener noreferrer"},"SQuAD")," specifically for question-answering tasks."),"\n",o.createElement(n.p,null,"Hugging Face pipelines provide an easy way to use models for inference. These pipelines abstract most of the complex code from the library, offering a simple API dedicated to various tasks, such as question answering or text generation."),"\n",o.createElement(n.p,null,"In our question-answering task, given a user query, the retriever fetches relevant documents from the knowledge source. The context from these documents is then passed to the question-answering model to generate an answer. See the example below:"),"\n",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline\n<span class="token keyword">import</span> torch\n\nqa_model <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"question-answering"</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">"distilbert/distilbert-base-cased-distilled-squad"</span><span class="token punctuation">,</span> torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></code></pre></div>'}}),"\n",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">question <span class="token operator">=</span> <span class="token string">"What is a good tyre width for bikepacking trips?"</span>\n\n<span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f\'Question: </span><span class="token interpolation"><span class="token punctuation">{</span>question<span class="token punctuation">}</span></span><span class="token string">\\n\'</span></span><span class="token punctuation">)</span>\n\ncontext_docs <span class="token operator">=</span> retriever<span class="token punctuation">.</span>invoke<span class="token punctuation">(</span>question<span class="token punctuation">)</span>\n\ncontext <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>doc<span class="token punctuation">.</span>page_content <span class="token keyword">for</span> doc <span class="token keyword">in</span> context_docs<span class="token punctuation">]</span><span class="token punctuation">)</span>\n\n<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f\'Using context from </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>context_docs<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string"> blog pages to answer the question:\\n\'</span></span><span class="token punctuation">)</span>\n\n<span class="token keyword">for</span> doc <span class="token keyword">in</span> context_docs<span class="token punctuation">:</span>\n    page_title <span class="token operator">=</span> doc<span class="token punctuation">.</span>metadata<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"_source"</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"metadata"</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"title"</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span>\n    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f\'- </span><span class="token interpolation"><span class="token punctuation">{</span>page_title<span class="token punctuation">}</span></span><span class="token string">\'</span></span><span class="token punctuation">)</span>\n\nqa_response <span class="token operator">=</span> qa_model<span class="token punctuation">(</span>question <span class="token operator">=</span> question<span class="token punctuation">,</span> context <span class="token operator">=</span> context<span class="token punctuation">)</span>\n\n<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f\'\\nAnswer: </span><span class="token interpolation"><span class="token punctuation">{</span>qa_response<span class="token punctuation">[</span><span class="token string">"answer"</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">\'</span></span><span class="token punctuation">)</span></code></pre></div>'}}),"\n",o.createElement(n.p,null,"Output:"),"\n",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="plaintext"><pre class="language-plaintext"><code class="language-plaintext">Question: What is a good tyre width for bikepacking trips?\n\nUsing context from 3 blog pages to answer the question:\n\n- Bikepacking in France - Provence and Hautes-Alpes — Jedr\'s Blog\n- Gravmageddon 2023: Karkonosze - Izery Gravel Race — Jedr\'s Blog\n- Tuscany Trail: Bikepacking in Italy — Jedr\'s Blog\n\nAnswer: 45mm</code></pre></div>'}}),"\n",o.createElement(n.p,null,"In the retrieved context blog posts (retrieved using kNN vector search), I discuss my bikepacking setup and tire width. In fact, my gravel bike is equipped with 45mm tires! You can check the context of the blog posts yourself to see how the answer was generated:"),"\n",o.createElement(n.ul,null,"\n",o.createElement(n.li,null,o.createElement(n.a,{href:"/bikepacking-in-provence-france/"},"Bikepacking in France - Provence and Hautes-Alpes")),"\n",o.createElement(n.li,null,o.createElement(n.a,{href:"/gravmageddon-2023-karkonosze-izery-gravel-race/"},"Gravmageddon 2023: Karkonosze - Izery Gravel Race")),"\n",o.createElement(n.li,null,o.createElement(n.a,{href:"/tuscany-trail-bikepacking-in-italy/"},"Tuscany Trail: Bikepacking in Italy")),"\n"),"\n",o.createElement(n.h2,{id:"text-generation-with-locally-running-llm",style:{position:"relative"}},o.createElement(n.a,{href:"#text-generation-with-locally-running-llm","aria-label":"text generation with locally running llm permalink",className:"anchor before"},o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Text generation with locally running LLM"),"\n",o.createElement(n.p,null,"We can also use a large language model to generate comprehensive answers to questions, much like ChatGPT. For this purpose, we can utilize the ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">text-generation</code>'}})," pipeline from HuggingFace. I used ",o.createElement(n.a,{href:"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta",target:"_blank",rel:"nofollow noopener noreferrer"},"HuggingFaceH4/zephyr-7b-beta")," with “just” 7 billion parameters."),"\n",o.createElement(n.p,null,"Similar to the question-answering example, we can pass a question and context as the user prompt, and the model can generate an answer based on that. Here is an example code:"),"\n",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">pipe <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"text-generation"</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">"HuggingFaceH4/zephyr-7b-beta"</span><span class="token punctuation">,</span> torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span> device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">)</span>\n\n<span class="token comment"># limit the number of context docs to retrieve to top 1 context doc for faster processing by LLM</span>\nretriever <span class="token operator">=</span> ElasticsearchRetriever<span class="token punctuation">.</span>from_es_params<span class="token punctuation">(</span>\n    index_name<span class="token operator">=</span>ES_INDEX_NAME<span class="token punctuation">,</span>\n    body_func<span class="token operator">=</span><span class="token keyword">lambda</span> query<span class="token punctuation">:</span> vector_query<span class="token punctuation">(</span>query<span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>\n    content_field<span class="token operator">=</span><span class="token string">"text"</span><span class="token punctuation">,</span>\n    url<span class="token operator">=</span>es_url<span class="token punctuation">,</span>\n    api_key<span class="token operator">=</span>es_api_key\n<span class="token punctuation">)</span></code></pre></div>'}}),"\n",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">question <span class="token operator">=</span> <span class="token string">"What is a good tyre width for bikepacking trips?"</span>\n\ncontext_docs <span class="token operator">=</span> retriever<span class="token punctuation">.</span>invoke<span class="token punctuation">(</span>question<span class="token punctuation">)</span>\ncontext <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>doc<span class="token punctuation">.</span>page_content <span class="token keyword">for</span> doc <span class="token keyword">in</span> context_docs<span class="token punctuation">]</span><span class="token punctuation">)</span>\n\n<span class="token comment"># We use the tokenizer\'s chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating</span>\nmessages <span class="token operator">=</span> <span class="token punctuation">[</span>\n    <span class="token punctuation">{</span>\n        <span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span>\n        <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"You are a helpful question answering chatbot, that tries to answer the question given the context."</span><span class="token punctuation">,</span>\n    <span class="token punctuation">}</span><span class="token punctuation">,</span>\n    <span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string-interpolation"><span class="token string">f\'Question: </span><span class="token interpolation"><span class="token punctuation">{</span>question<span class="token punctuation">}</span></span><span class="token string">, context: </span><span class="token interpolation"><span class="token punctuation">{</span>context<span class="token punctuation">}</span></span><span class="token string">\\n\'</span></span><span class="token punctuation">}</span><span class="token punctuation">,</span>\n<span class="token punctuation">]</span>\nprompt <span class="token operator">=</span> pipe<span class="token punctuation">.</span>tokenizer<span class="token punctuation">.</span>apply_chat_template<span class="token punctuation">(</span>messages<span class="token punctuation">,</span> tokenize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> add_generation_prompt<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>\noutputs <span class="token operator">=</span> pipe<span class="token punctuation">(</span>prompt<span class="token punctuation">,</span> max_new_tokens<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> do_sample<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> temperature<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">,</span> top_k<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> top_p<span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">)</span>\n<span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"generated_text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre></div>'}}),"\n",o.createElement(n.p,null,"Output of RAG pipeline with LLM used for answer generation:"),"\n",o.createElement(n.blockquote,null,"\n",o.createElement(n.p,null,"Based on the provided context, a good tire width for bikepacking trips would be around 45mm, as the author’s Canyon Grizl AL 6 bike is equipped with 45mm wide tires. This tire width provides a good balance between comfort, traction, and speed on various terrain types commonly encountered during bikepacking trips. However, tire choice ultimately depends on personal preference, the specific terrain being tackled, and the rider’s riding style. It’s always recommended to test different tire widths and tread patterns in training rides before embarking on a long bikepacking trip."),"\n"),"\n",o.createElement(n.p,null,"Below is the full system prompt, togerher with user prompt (question + context) and the generated answer. The fact that it’s all running locally is so cool!"),"\n",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<div class="gatsby-highlight" data-language="plaintext"><pre class="language-plaintext"><code class="language-plaintext">&lt;|system|>\nYou are a helpful question answering chatbot, that tries to answer the question given the context.&lt;/s>\n&lt;|user|>\nQuestion: What is a good tyre width for bikepacking trips?, context: you feel like you’ve cycled to another planet when you reach this part of the ascent. Bikepacking Setup Bike The bike model is a Canyon Grizl AL 6 with 45mm wide tyres. Bags: 16.5L Ortlieb seatpack - I had all my camping gear and off-bike clothes there. You can fit some baguettes on top of it! 11L Ortlieb QR handlebar bag - I stored there my cycling clothes, warm and waterproof layers, and a camera in case of rain. 4.5L frame bag - I stored the bike repair kit and toothbrush, soap, etc. 2 x 5L fork bags - I was able to fit my tent and some random stuff there. The tent poles were strapped to a fork as well. 1L Apidura top tube bag - I love it for its magnetic rivets to quickly access important stuff. I kept my phone and documents there. 2 x 1L snack packs - One to hold the telephoto lens, the other to carry a bottle of wine when needed. Gear Camping Tent: Big Agnes Copper Spur HV UL2. At ~1400g with its packable size, this tent has been accompanying me on bikepacking trips for the past few years. Sleeping Bag: Cumulus 0°C comfort. It weighs 850g. It’s a bit bulky, but I prefer to be a bit too warm rather than too cold at night. It fits well into the seat pack. Other camping gear: Stuff from Decathlon with a good price-to-weight ratio. Photo Fujifilm X-S10 (465g) Fujinon 18-55mm F/2.8-4 is not the sharpest lens, but at 310 g, it has a good performance-to-weight ratio. Fujinon 70-300mm F/4-5.6, my all-time favourite lens, has great sharpness and a decent focal length range at a weight of 580g. Actually, I like the fact that the lens is made out of quality plastic rather than metal because it makes it lighter. Mini-tripod for night shots: the ballhead taken from a regular tripod was the heaviest part. DIY 3-point camera strap compatible with peak design quick-release anchor links, tailored for cycling with a camera on my back. Chain Waxing and Bikepacking I’ve been waxing the chain on my road bike for almost a year, and\n&lt;/s>\n&lt;|assistant|>\nBased on the provided context, a good tire width for bikepacking trips would be around 45mm, as the author\'s Canyon Grizl AL 6 bike is equipped with 45mm wide tires. This tire width provides a good balance between comfort, traction, and speed on various terrain types commonly encountered during bikepacking trips. However, tire choice ultimately depends on personal preference, the specific terrain being tackled, and the rider\'s riding style. It\'s always recommended to test different tire widths and tread patterns in training rides before embarking on a long bikepacking trip.</code></pre></div>'}}),"\n",o.createElement(n.h2,{id:"how-can-llm-run-locally-in-a-notebook",style:{position:"relative"}},o.createElement(n.a,{href:"#how-can-llm-run-locally-in-a-notebook","aria-label":"how can llm run locally in a notebook permalink",className:"anchor before"},o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"How can LLM run locally in a notebook?!"),"\n",o.createElement(n.p,null,"Apple Silicon features a unified memory architecture that shares memory between the CPU and GPU. This design allows both the CPU and GPU to access the same data without needing to copy it between separate memory pools. As a result, if the model fits within the available RAM, both the CPU and GPU can work efficiently without significant bottlenecks."),"\n",o.createElement(n.p,null,"To optimize memory usage, we set the model to use ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">torch.bfloat16</code>'}})," precision, which reduces the memory required by using a 16-bit format instead of the default 32-bit. This reduces the memory footprint of our 7-billion-parameter model, ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">HuggingFaceH4/zephyr-7b-beta</code>'}}),", to about 14GB, allowing it to fit comfortably on my 32GB M2 Pro MacBook. With reasonably short prompts, it performs almost as fast as what you would get from the ChatGPT app."),"\n",o.createElement(n.p,null,"Out of curiosity, I explored what happens if the model does not fit in memory. I created two instances of our 7-billion-parameter model and, as expected, received a warning: ",o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<code class="language-text">Some parameters are on the meta device because they were offloaded to the disk</code>'}}),". Offloading the model to disk resulted in the model becoming unusably slow. I eventually gave up and restarted the kernel after waiting for a response that never came."),"\n",o.createElement(n.h2,{id:"notebook",style:{position:"relative"}},o.createElement(n.a,{href:"#notebook","aria-label":"notebook permalink",className:"anchor before"},o.createElement(n.span,{dangerouslySetInnerHTML:{__html:'<svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg>'}})),"Notebook"),"\n",o.createElement(n.p,null,"All of the steps listed above are availabe in the ",o.createElement(n.a,{href:"https://gist.github.com/jedrazb/31d65cf0b49a94080000cd8f3432b4e7",target:"_blank",rel:"nofollow noopener noreferrer"},"Python notebook"),"."))}var l=function(e){void 0===e&&(e={});const{wrapper:n}=Object.assign({},(0,s.R)(),e.components);return n?o.createElement(n,e,o.createElement(r,e)):r(e)};var p=a(4794),c=a(8156),i=a.n(c),u=a(2532),d=a(39),h=a(9203),g=a(2907),k=a(9379),m=a(5181),f=a(3303),y=a(4799),b=a(1863),v=a(7821),w=a(5765),_=a(4039),E=a(4310);const x={Link:p.Link,ImageGallery:k.A,ImageComponent:m.A,Container:b.mc,Column:b.VP,MakeItBigContainer:b.r,ThreePhotosContainer:b.Rq,LazyIframe:f.A,StatefulSliderPicker:_.a,StatefulBlockPicker:_.A};let S=function(e){function n(){return e.apply(this,arguments)||this}return(0,t.A)(n,e),n.prototype.render=function(){const{children:e}=this.props,n=this.props.data.mdx,a=i()(this.props,"data.site.siteMetadata.title"),t=i()(this.props,"data.site.siteMetadata.siteUrl");let{previous:r,next:l}=this.props.pageContext;const c=n.frontmatter.ogimage,k=c&&(0,u.d)(c),m=i()(n,"fields.category"),f=t+"/"+m+n.fields.slug,b={"@context":"https://schema.org","@type":"BlogPosting",headline:n.frontmatter.title,datePublished:n.frontmatter.date,url:f,author:[{"@type":"Person",name:"Jedr Blaszyk",url:"https://j.blaszyk.me/"}]};return o.createElement(h.A,{location:this.props.location,title:a,tocComponent:o.createElement(E.A,n.tableOfContents)},o.createElement(g.A,{title:n.frontmatter.title,description:n.frontmatter.spoiler,slug:n.fields.slug,image:k,structuredData:b}),o.createElement("main",null,o.createElement("article",{className:"post"},o.createElement("header",{id:"post-header"},o.createElement("h1",{style:{color:"var(--textTitle)",marginTop:"1.5rem",marginBottom:"0.5rem"}},n.frontmatter.title),o.createElement(p.Link,{style:{boxShadow:"none",textDecoration:"none",color:"var(--textLink)",fontFamily:"Montserrat, sans-serif"},to:"/tech-blog/",rel:"bookmark"},o.createElement("p",null,"Tech Blog")),o.createElement("p",{style:{...(0,w.hs)(-.2),display:"block",marginBottom:(0,w.di)(1),marginTop:(0,w.di)(-.8)}},(0,v.Wy)(n.frontmatter.date),o.createElement("span",{style:{margin:"0 0.15rem"}}," • "),(0,v.Bt)(n.fields.timeToRead.minutes))),o.createElement(s.x,{components:x},e))),o.createElement("aside",null,o.createElement("nav",null,o.createElement("ul",{style:{display:"flex",flexWrap:"wrap",justifyContent:"space-between",listStyle:"none",padding:0,marginLeft:0}},o.createElement("li",null,r&&o.createElement(p.Link,{to:"/"+m+r.fields.slug,rel:"prev"},"← ",r.frontmatter.title)),o.createElement("li",null,l&&o.createElement(p.Link,{to:"/"+m+l.fields.slug,rel:"next"},l.frontmatter.title," →")))),o.createElement("h3",{style:{fontFamily:"Montserrat, sans-serif",marginTop:(0,w.di)(.25)}},o.createElement(p.Link,{style:{boxShadow:"none",textDecoration:"none",color:"var(--textLink)",fontSize:(0,w.di)(.8)},to:"/"},"Jedr's Blog")," • ",o.createElement(p.Link,{style:{boxShadow:"none",textDecoration:"none",color:"var(--textLink)",fontSize:(0,w.di)(.8)},to:"/tech-blog/"},"Tech Blog")),o.createElement(d.A),o.createElement(y.A,{url:f,id:n.fields.slug,title:n.frontmatter.title})))},n}(o.Component);function H(e){return o.createElement(S,e,o.createElement(l,e))}}}]);
//# sourceMappingURL=component---src-templates-tech-blog-post-js-content-file-path-content-tech-blog-question-answering-with-langchain-huggingface-and-elasticsearch-index-mdx-9c4677a8a284c2a8794b.js.map